# -*- coding: utf-8 -*-
"""DIR_CIFAR10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1up88wpQPoAHOF7u-2toR6GpS8LTgxCFf
"""

import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras import layers, Model
from tensorflow.keras.layers import Conv2D, Convolution2DTranspose, MaxPooling2D, UpSampling2D, LeakyReLU, BatchNormalization
from tensorflow.keras.layers import Conv2DTranspose, MaxPooling2D, UpSampling2D, LeakyReLU, Flatten, Dense, Reshape
from tensorflow.keras.activations import relu
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt



# load cifar10 dataset
(_, _), (x_val, y_val) = cifar10.load_data()

# batch size
batch_size = 100

# show some training images
fig = plt.figure(figsize = (20, 4))

for idx in range(20):
  ax = fig.add_subplot(2, 10, idx + 1, xticks = [], yticks = [])
  plt.imshow(x_val[idx], cmap = 'gray')

x_val = x_val[:1000]
y_val = y_val[:1000]

print("Number of Validation Samples: ", len(x_val))

# shape of images
print("Shape of Validation images: ", x_val.shape)

# change data type
x_val = x_val.astype('float32')

# normalize data into range 1-0
x_val /= 255.0

print("Validation shape: ", x_val.shape)

from google.colab import drive
drive.mount('/content/drive')

import pickle
fgsm_test = open('./drive/My Drive/PhD_Thesis/adversarial_noise/cifar10/cifar10_fgsm_0.045_test.pickle', 'rb')
fgsm_noise_test = pickle.load(fgsm_test)



fgsm_noise_test.shape
plt.imshow(fgsm_noise_test[0])



x_val_noise = x_val + fgsm_noise_test[:1000]
x_val_noise.shape

# show some training images
fig = plt.figure(figsize = (20, 4))

for idx in range(20):
  ax = fig.add_subplot(2, 10, idx + 1, xticks = [], yticks = [])
  plt.imshow(np.clip(x_val_noise[idx], 0, 1), cmap = 'gray')



# define encoder
  
inputs = layers.Input(shape = [32, 32, 3])
x = inputs

x = Conv2D(filters = 128, kernel_size = (3, 3), strides = 2, padding = 'same', activation = relu)(x)
x = BatchNormalization()(x)
  
x = Conv2D(filters = 256, kernel_size = (3, 3), strides = 2, padding = 'same', activation = relu)(x)
x = BatchNormalization()(x)

x = Conv2D(filters = 512, kernel_size = (3, 3), strides = 2, padding = 'same', activation = relu)(x)
x = Conv2D(filters = 512, kernel_size = (3, 3), strides = 1, padding = 'same', activation = relu)(x)
x = BatchNormalization()(x)

x = Conv2D(filters = 1024, kernel_size = (3, 3), strides = 2, padding = 'same', activation = relu)(x)
x = Conv2D(filters = 1024, kernel_size = (3, 3), strides = 1, padding = 'same', activation = relu)(x)
x = BatchNormalization()(x)

x = Flatten()(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = Dense(units = 4096, activation = relu)(x)

# decoder
x = Reshape((2, 2, 1024))(x)
x = Conv2DTranspose(filters = 1024, kernel_size = (3, 3), strides = 2, padding = 'same', activation = relu)(x)
x = Conv2D(filters = 1024, kernel_size = (3, 3), strides = 1, padding = 'same', activation = relu)(x)
x = BatchNormalization()(x)

x = Conv2DTranspose(filters = 512, kernel_size = (3, 3), strides = 2, padding = 'same', activation = relu)(x)
x = Conv2D(filters = 512, kernel_size = (3, 3), strides = 1, padding = 'same', activation = relu)(x)
x = BatchNormalization()(x)

x = Conv2DTranspose(filters = 256, kernel_size = (3, 3), strides = 2, padding = 'same', activation = relu)(x)
x = Conv2D(filters = 256, kernel_size = (3, 3), strides = 1, padding = 'same', activation = relu)(x)
x = BatchNormalization()(x)

x = Conv2DTranspose(filters = 128, kernel_size = (3, 3), strides = 2, padding = 'same', activation = relu)(x)
x = Conv2D(filters = 128, kernel_size = (3, 3), strides = 1, padding = 'same', activation = relu)(x)
x = BatchNormalization()(x)

x = tf.keras.layers.Dropout(0.2)(x)
out = Conv2D(filters = 3, kernel_size = (3, 3), strides = 1, padding = 'same', activation = 'sigmoid')(x)

DIR_cifar10 = Model(inputs = inputs, outputs = out)

print(DIR_cifar10.summary())

model_path = './content/drive/MyDrive/PhD_Thesis/Generative_Models/DIR_fgsm.h5'
checkpoint = ModelCheckpoint(model_path, 
                             monitor= "val_loss", 
                             verbose=1, 
                             save_best_only=True, 
                             mode= "auto", 
                             save_weights_only = False)

early = EarlyStopping(monitor= "val_loss", 
                      mode= "auto", 
                      patience = 20)

callbacks_list = [checkpoint, early]

DIR_cifar10.compile(loss = 'mse', optimizer = tf.keras.optimizers.RMSprop(learning_rate = 3e-4))

DIR_cifar10.fit(x_val_noise, x_val,
                      validation_data = (x_val_noise, x_val),
                      epochs = 300, batch_size = 100, callbacks = callbacks_list)

# sample and display images
dir_model = tf.keras.models.load_model('./content/drive/MyDrive/PhD_Thesis/Generative_Models/DIR_fgsm.h5')
outputs = dir_model.predict(x_val_noise)

# display the 1st 8 test input and decoded images
imgs = np.concatenate([x_val[:12], x_val_noise[:12], outputs[:12]])
imgs = imgs.reshape((3, 12, 32, 32, 3))

imgs = np.vstack([np.hstack(i) for i in imgs])
plt.figure(figsize = (20, 6))
plt.axis('off')
plt.title('Input: 1st 2 rows, Decoded: last 2 rows')
plt.imshow(imgs, interpolation='none')

fig = plt.figure(figsize = (20, 4))

for idx in range(20):
  ax = fig.add_subplot(2, 10, idx + 1, xticks = [], yticks = [])
  plt.imshow(np.clip(outputs[idx], 0, 1), cmap = 'gray')

